{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":".ipynb","provenance":[],"authorship_tag":"ABX9TyNvXSnYlQSmIqLlI7qCMTEH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### To save it in drive"],"metadata":{"id":"IzWv236Ajt3s"}},{"cell_type":"code","execution_count":97,"metadata":{"id":"KgKc1vwP6-K6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650302209889,"user_tz":-120,"elapsed":1616,"user":{"displayName":"Eva Laye","userId":"10337073958610039418"}},"outputId":"2fdcefd6-a45d-487c-a356-dcf8c7f9af0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["my_local_drive= '/content/drive/My Drive/TER_M1/'\n","import sys\n","sys.path.append(my_local_drive)\n","%cd $my_local_drive"],"metadata":{"id":"Q33aauT_qw7h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650302209893,"user_tz":-120,"elapsed":11,"user":{"displayName":"Eva Laye","userId":"10337073958610039418"}},"outputId":"901c1954-82a0-49fd-b760-4242c1bda05a"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/TER_M1\n"]}]},{"cell_type":"markdown","source":["# The main project"],"metadata":{"id":"MJEBcR9ljy00"}},{"cell_type":"code","source":["####  Load imports\n","import os\n","import string\n","import matplotlib\n","import matplotlib_inline\n","import numpy as np\n","import pandas as pd\n","import gensim.models\n","import nltk\n","import matplotlib.pyplot as plt\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","\n","matplotlib_inline\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"id":"r1zYL0gn8K9v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" In order to process this data model, we propose to consider each table as a symbol document and the set of tables as a collection of documents. A symbol can be a table’s name, a table’s attribute, or a relationship between two table (foreign key).\n","\n","We model it by repeating (4 times in this case) the foreign key and the name of the source document in the target documents. We do the same for the name of the target documents in the source document.\n"],"metadata":{"id":"1-pb0iz9kfiE"}},{"cell_type":"code","source":["# ........... read documents.........................\n","data = open(\"document_construction.txt\", \"r\")"],"metadata":{"id":"BO6Zo83n8T_Y","executionInfo":{"status":"ok","timestamp":1650302518912,"user_tz":-120,"elapsed":278,"user":{"displayName":"Eva Laye","userId":"10337073958610039418"}}},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing"],"metadata":{"id":"IETPNfAhkQzS"}},{"cell_type":"code","source":["#    stop words\n","stop_words = set(stopwords.words('english'))\n","\n","# punctuation\n","punctuation = set(string.punctuation)\n","\n","# lemmatization\n","\n","lemmatization = WordNetLemmatizer()"],"metadata":{"id":"H08Ixf6t8eaA","executionInfo":{"status":"ok","timestamp":1650302522430,"user_tz":-120,"elapsed":282,"user":{"displayName":"Eva Laye","userId":"10337073958610039418"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["# function clean\n","def clean(documents):\n","    # split documents and remove stop words\n","\n","    split_doc = \" \".join([i for i in documents.lower().split() if i not in stop_words])\n","\n","    # remove punctuation\n","    punc_doc = ''.join([j for j in split_doc if j not in punctuation])\n","\n","    # normalize the text\n","    normalized = \" \".join([lemmatization.lemmatize(word) for word in punc_doc.split()])\n","\n","    return normalized"],"metadata":{"id":"nIwrP6yT8gKW","executionInfo":{"status":"ok","timestamp":1650302522705,"user_tz":-120,"elapsed":33,"user":{"displayName":"Eva Laye","userId":"10337073958610039418"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["# clean documents in the file text\n","\n","corpus = data\n","clean_documents = [clean(doc) for doc in corpus]"],"metadata":{"id":"v2OnHVLJ8lq4","executionInfo":{"status":"ok","timestamp":1650302522710,"user_tz":-120,"elapsed":32,"user":{"displayName":"Eva Laye","userId":"10337073958610039418"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["# Tables names \n","Table_name = []\n","\n","for doc in clean_documents:\n","     d=0\n","     if d==0:\n","\n","       t= doc.split()[0] +\" \" +doc.split()[1]\n","       if doc.split()[0]!= doc.split()[1]:\n","\n","           Table_name.append(t)\n","       else:\n","           Table_name.append(doc.split()[0])\n","       d=d+1"],"metadata":{"id":"tMnEtCi-8xTO","executionInfo":{"status":"ok","timestamp":1650302522715,"user_tz":-120,"elapsed":31,"user":{"displayName":"Eva Laye","userId":"10337073958610039418"}}},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":["### CLUSTERING"],"metadata":{"id":"-p5C5lAorx_P"}},{"cell_type":"markdown","source":["TF-IDF Vectorization"],"metadata":{"id":"0xDBmlwElaCl"}},{"cell_type":"code","source":["tfidf_vectorizer = TfidfVectorizer(lowercase=False)\n","\n","tf_idf = tfidf_vectorizer.fit_transform(clean_documents)\n","tf_idf"],"metadata":{"id":"CZMhvM__85CP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650302553643,"user_tz":-120,"elapsed":260,"user":{"displayName":"Eva Laye","userId":"10337073958610039418"}},"outputId":"4eb8b603-c6d5-44d8-c2eb-2674bea92c8f"},"execution_count":120,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<14x96 sparse matrix of type '<class 'numpy.float64'>'\n","\twith 162 stored elements in Compressed Sparse Row format>"]},"metadata":{},"execution_count":120}]},{"cell_type":"code","source":["#  view ressult\n","feature_names = tfidf_vectorizer.get_feature_names()\n","dense = tf_idf.todense()\n","denselist = dense.tolist()\n","\n","all_key_words = []\n","\n","for d in denselist:\n","    i = 0\n","    kwords = []\n","\n","    for word in d:\n","        if word > 0:\n","            kwords.append(feature_names[i])\n","        i = i + 1\n","    all_key_words.append(kwords)"],"metadata":{"id":"kusyrg4asbje"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Elbow Method"],"metadata":{"id":"AdkDJ_-Nl1jJ"}},{"cell_type":"code","source":["wcss = []  # WCSS Scores = within-cluster-sum of squared distance\n","clusters_1 = range(1, 12)\n","for i in clusters_1:\n","    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=30, algorithm=\"elkan\")\n","    kmeans.fit(tf_idf)\n","    wcss.append(kmeans.inertia_)"],"metadata":{"id":"3c2-xn1A9tXE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(clusters_1, wcss)\n","plt.title('Elbow Method')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('Sum of Squared Distance')\n","plt.show()"],"metadata":{"id":"JpAGfnqVmYmA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implementation of KMeans"],"metadata":{"id":"hsvCV2kQmNRL"}},{"cell_type":"code","source":["clusters = 5  # modified for Petclinc data will be 3 or 2\n","\n","model = KMeans(n_clusters=clusters, init=\"k-means++\", max_iter=300, n_init=30, algorithm=\"elkan\")\n","\n","model.fit(tf_idf)\n","\n","order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n","\n","terms = tfidf_vectorizer.get_feature_names()\n","\n","labels = model.labels_"],"metadata":{"id":"DduLaQSg9uoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(clusters):\n","  print(f\" \\n Cluster {i} \\n\")\n","  for ind in order_centroids[i, :10]:\n","      print(terms[ind])"],"metadata":{"id":"1fD0CoIw94Wb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Obtained Clusters\n","\n","obtainesCluster =[]\n","print(\"Obtained Clusters: \\n\")\n","for c in range(clusters):\n","    print(\" Cluster\", c)\n","    print(\"__________________________________________\")\n","    count = 0\n","    cluseterI =[]\n","    cluseterI.append(c)\n","    for i in labels:\n","\n","        if i == c:\n","            print('Table: {} ---> Document {}'.format(Table_name[count], count))\n","            cluseterI.append(Table_name[count])\n","        count = count + 1\n","\n","    obtainesCluster.append(cluseterI)\n","    print(\"__________________________________________\")"],"metadata":{"id":"PVYyqUW89_2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\" test:\")\n","cc=0\n","for oc in obtainesCluster:\n","    print(\"\\n cluseter\",cc,\" -->\",oc)\n","    cc= cc +1"],"metadata":{"id":"ciKTpuqW-J9T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using PCA to reduces the dimensionality of a data set to an arbitrary number while preserving most of the information contained in it"],"metadata":{"id":"ZUzXMEv9mr8R"}},{"cell_type":"code","source":["pca = PCA(n_components=2)\n","\n","scatter_plot_points = pca.fit_transform(tf_idf.toarray())\n","\n","colors = [\"r\", \"b\", \"c\", \"y\", \"m\", \"g\"]\n","\n","X_axe = [i[0] for i in scatter_plot_points]\n","Y_axe = [i[1] for i in scatter_plot_points]\n","\n","plt.scatter(X_axe, Y_axe, c=[colors[d] for d in labels])\n","plt.show()"],"metadata":{"id":"oisTZxBV-YaT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using seaborn visualization\n"],"metadata":{"id":"GkOfwI0opMr5"}},{"cell_type":"code","source":["import seaborn as sns\n","\n","plt.figure(figsize=(10, 6))     # set image size\n","plt.title(\"Clusters\", fontdict={\"fontsize\": 18})\n","\n","# set axes names\n","plt.xlabel(\"X0\", fontdict={\"fontsize\": 16})\n","plt.ylabel(\"X1\", fontdict={\"fontsize\": 16})\n","\n","sns.scatterplot(data=tf_idf.toarray(), x=X_axe , y=Y_axe, palette=\"viridis\",c=[colors[d] for d in labels])\n","plt.show()"],"metadata":{"id":"jGsCPYyenCgL"},"execution_count":null,"outputs":[]}]}